# üöÄ Escalamiento B√°sico con HPA (`exp1_basic-autoscaling`)

Este documento describe el flujo completo del primer experimento de elasticidad, en el cual se eval√∫a c√≥mo un sistema Kubernetes responde a una carga progresiva utilizando el Horizontal Pod Autoscaler (HPA). 

El experimento simula una aplicaci√≥n web b√°sica (NGINX) desplegada en un cl√∫ster, expuesta a una carga generada por usuarios virtuales mediante k6. El objetivo es observar c√≥mo var√≠a el n√∫mero de r√©plicas del microservicio en respuesta al uso de CPU, analizar la relaci√≥n entre la demanda estimada y la oferta disponible de recursos, y construir curvas de elasticidad que reflejen la capacidad adaptativa del sistema.

Este experimento es el punto de partida para analizar el comportamiento de escalado horizontal autom√°tico bajo condiciones controladas, utilizando configuraciones m√≠nimas para facilitar la observaci√≥n y an√°lisis del sistema.

---

## üéØ Objetivo

El prop√≥sito de este experimento es analizar el comportamiento de un sistema Kubernetes que implementa escalamiento autom√°tico horizontal (Horizontal Pod Autoscaler - HPA) bajo una carga progresiva y controlada. A trav√©s de este proceso se busca:

- Observar la din√°mica de escalado del sistema en respuesta a cambios en el uso de CPU.
- Registrar eventos clave de escalamiento (`scaleup` y `scaledown`) generados por el HPA.
- Estimar la demanda te√≥rica de CPU en funci√≥n de la carga generada (usuarios virtuales o requests).
- Calcular la oferta efectiva de CPU proporcionada por los pods activos durante la prueba.
- Construir curvas de elasticidad que permitan comparar demanda y oferta de recursos a lo largo del tiempo.
- Identificar posibles situaciones de subaprovisionamiento o sobreaprovisionamiento.
- **Calcular m√©tricas cuantitativas de elasticidad**, tales como:
  - Precisi√≥n promedio del escalado hacia arriba y hacia abajo.
  - Tiempos acumulados y promedios en estados de sub/sobreaprovisionamiento.
  - Recursos promedio utilizados o desperdiciados en dichos estados.
  - Elasticidad total del sistema, considerando tanto capacidad como tiempo de respuesta.

Este an√°lisis permite evaluar qu√© tan efectiva es la configuraci√≥n b√°sica del HPA para responder a fluctuaciones de carga y qu√© tan bien se adapta la infraestructura a la demanda.

---

## üß© Arquitectura del experimento

Este experimento se ejecuta en un entorno Kubernetes, donde se despliegan los siguientes componentes:

- **Microservicio (NGINX)**: Servicio simulado que act√∫a como blanco de la carga. Se usa una imagen ligera de NGINX configurada con l√≠mites y solicitudes (`requests`) de CPU definidos para permitir que el HPA act√∫e correctamente.

- **Horizontal Pod Autoscaler (HPA)**: Controlador nativo de Kubernetes que monitorea el uso promedio de CPU de los pods en ejecuci√≥n y ajusta autom√°ticamente el n√∫mero de r√©plicas para mantener el uso cerca de un objetivo configurado. En este experimento, el objetivo es del 25%.

- **Generador de carga (k6)**: Herramienta de pruebas de carga que simula usuarios virtuales (VUs) generando solicitudes HTTP al microservicio durante un periodo de tiempo. La carga se define en etapas para observar c√≥mo responde el sistema a incrementos y reducciones.

- **Recolectores de datos (scripts Bash)**: Capturan m√©tricas relevantes durante la ejecuci√≥n del experimento:
  - Uso de CPU por pod.
  - N√∫mero de r√©plicas activas.
  - Eventos de escalamiento registrados por Kubernetes.

- **M√≥dulo de an√°lisis (scripts Python + contenedor Docker)**: Procesa los datos recolectados, sincroniza eventos y m√©tricas, y genera gr√°ficos y m√©tricas de elasticidad.

### Diagrama de interacci√≥n

```
       +------------------+
       | Generador de     |
       | carga (k6)       |
       +--------+---------+
                |
                v
       +--------+---------+
       | Microservicio    |
       | (NGINX)          |
       +--------+---------+
                |
                v
       +--------+---------+
       | Pods del         |
       | Deployment       |
       +--------+---------+
                |
       (Reportan uso de CPU)
                |
                v
       +------------------+
       | Autoscaler (HPA) |
       | Target: 25% CPU  |
       +--------+---------+
                |
       (Escala n√∫mero de pods)
                |
                v
       +------------------------+
       | Registro de eventos   |
       | (Kubernetes Event Log)|
       +--------+--------------+
                |
                v
       +------------------------+
       | Recolectores de datos |
       | y scripts de an√°lisis |
       +------------------------+
```

Este dise√±o modular permite ejecutar y analizar el experimento de forma automatizada, controlada y reproducible.

## üß™ Flujo del experimento paso a paso

A continuaci√≥n se describe el proceso completo que ocurre durante la ejecuci√≥n del experimento, desde el despliegue hasta la generaci√≥n de resultados. Cada paso representa una acci√≥n concreta del sistema o de los scripts que forman parte de la infraestructura experimental.

| Paso | Descripci√≥n t√©cnica |
|------|---------------------|
| 1Ô∏è‚É£   | **Despliegue del microservicio y del HPA**: Se aplican los manifiestos YAML (`deployment.yaml` y `hpa.yaml`) que crean el Deployment de NGINX con sus l√≠mites de CPU y el HPA con su objetivo de 25% de utilizaci√≥n promedio. |
| 2Ô∏è‚É£   | **Inicializaci√≥n de recolectores de datos**: Se ejecutan en segundo plano dos scripts Bash: uno para capturar m√©tricas peri√≥dicas de uso de CPU y n√∫mero de pods (`metric_collector_basic.sh`), y otro para registrar eventos de escalamiento desde el cl√∫ster (`capture_deployment_events.sh`). |
| 3Ô∏è‚É£   | **Ejecuci√≥n de la carga con k6**: Se lanza el script de prueba (`basic_autoscaling_test.js`) que genera carga HTTP con diferentes niveles de usuarios virtuales (VUs) durante una ventana de tiempo de 7 minutos, provocando variaciones en el uso de CPU. |
| 4Ô∏è‚É£   | **Monitoreo del comportamiento del HPA**: Kubernetes recolecta las m√©tricas de CPU de los pods y el HPA toma decisiones de escalamiento, aumentando o reduciendo r√©plicas para mantener la utilizaci√≥n cercana al 25%. Estas decisiones quedan registradas como eventos. |
| 5Ô∏è‚É£   | **Finalizaci√≥n y recolecci√≥n de datos**: Una vez terminada la carga, los recolectores se detienen y todos los datos se consolidan en archivos CSV y JSON en la carpeta `output/`. |
| 6Ô∏è‚É£   | **Procesamiento y an√°lisis**: Se ejecutan scripts Python desde un contenedor Docker que procesan las m√©tricas, sincronizan los eventos con los datos de CPU, generan los gr√°ficos de comportamiento y calculan las m√©tricas de elasticidad. |

Este flujo es ejecutado manualmente o de forma autom√°tica mediante el script `exp1_basic-autoscaling.sh`, lo que permite replicar el experimento de forma consistente.

---

## üì¶ Archivos y configuraciones

El experimento se apoya en varios archivos clave organizados en carpetas espec√≠ficas. Cada uno cumple un rol dentro del proceso de despliegue, recolecci√≥n de datos, generaci√≥n de carga y an√°lisis.

### üóÇÔ∏è `manifests/` - Manifiestos de Kubernetes

- `deployment.yaml`: Define el Deployment de NGINX, especificando la imagen utilizada, el n√∫mero inicial de r√©plicas, los `resource requests` y `limits` de CPU, as√≠ como los probes necesarios.
- `hpa.yaml`: Contiene la configuraci√≥n del Horizontal Pod Autoscaler (HPA), con un objetivo de utilizaci√≥n promedio de CPU del 25%, y un rango de r√©plicas entre 1 y 10.

### üóÇÔ∏è `scripts/` - Generaci√≥n de carga y captura de datos

- `basic_autoscaling_test.js`: Script de k6 que define un escenario de carga dividido en etapas. Simula la actividad de usuarios virtuales con un patr√≥n escalonado: subida, carga sostenida, bajada y finalizaci√≥n.
- `capture_deployment_events.sh`: Ejecuta un ciclo peri√≥dico para registrar eventos del tipo `ScalingReplicaSet` generados por el HPA. Estos eventos permiten identificar cu√°ndo y cu√°ntas r√©plicas fueron a√±adidas o eliminadas.
- `metric_collector_basic.sh`: Utiliza `kubectl top pods` y `kubectl get deployment` para recolectar el uso de CPU por pod y el n√∫mero de r√©plicas activas, generando un log cada 10 segundos.

### üîß Carga generada (en `basic_autoscaling_test.js`)

La carga simulada se define por etapas en el script de k6. Cada etapa especifica cu√°ntos usuarios virtuales (VUs) estar√°n activos y por cu√°nto tiempo:

```js
stages: [
  { duration: '1m', target: 50 },   // Subida progresiva de carga
  { duration: '3m', target: 150 },  // Carga alta sostenida (deber√≠a activar el HPA)
  { duration: '2m', target: 50 },   // Reducci√≥n progresiva
  { duration: '1m', target: 0 },    // Descenso total de la carga
]
```

### üóÇÔ∏è `analysis/` - An√°lisis de datos y visualizaci√≥n (ubicado en `files/basic-autoscaling/analysis/`)

Esta carpeta contiene los scripts de Python encargados del procesamiento de los datos crudos recolectados durante el experimento. Tambi√©n incluye herramientas para graficar la evoluci√≥n del sistema y calcular m√©tricas de elasticidad.

#### Scripts principales:

- `filter_scaling_events.py`: Filtra los eventos de escalamiento del archivo crudo (`scaling_events.csv`) y los sincroniza temporalmente con el inicio de la carga, generando una versi√≥n limpia (`scaling_events_clean.csv`).

- `plot_cpu_usage.py`: Genera gr√°ficos del uso de CPU por pod a lo largo del tiempo. √ötil para observar carga distribuida entre r√©plicas.

- `plot_cpu_usage_with_events.py`: Variante del anterior que incluye l√≠neas verticales que marcan los momentos de escalamiento (`scaleup`, `scaledown`).

- `plot_pod_count.py`: Grafica la evoluci√≥n del n√∫mero de r√©plicas activas del microservicio durante el experimento.

- `plot_pod_count_with_events.py`: Incluye eventos de escalamiento en la gr√°fica anterior.

- `plot_elasticity_curve.py`: Construye la curva de elasticidad comparando la demanda estimada de CPU (por VUs o requests) con la oferta disponible basada en el n√∫mero de pods.

- `plot_elasticity_curve_with_events.py`: Variante con anotaciones de eventos de escalamiento para enriquecer el an√°lisis visual.

- `calculate_elasticity_metrics.py`: Calcula un conjunto completo de m√©tricas de elasticidad, incluyendo precisi√≥n de escalamiento, tiempos y recursos en estados de sub/sobreaprovisionamiento, y elasticidad global.

- `plot_indirect_elasticity_metrics.py`: Visualiza m√©tricas de elasticidad complementarias para facilitar su interpretaci√≥n.

#### Archivos auxiliares:

- `Dockerfile`: Define el entorno reproducible para ejecutar los scripts de an√°lisis desde un contenedor. Incluye todas las dependencias necesarias.

- `requirements.txt`: Lista de paquetes de Python requeridos (pandas, matplotlib, etc.) para ejecutar los scripts sin errores.

- `images/`: Carpeta donde se guardan autom√°ticamente todos los gr√°ficos generados por los scripts.

> üìÇ Todos estos archivos est√°n ubicados en:  
> `files/basic-autoscaling/analysis/`  
> y el presente documento Markdown se encuentra en la ra√≠z de `files/basic-autoscaling/`.

---

## üîÅ Requisito previo: ejecutar el microbenchmark

Antes de ejecutar este experimento de autoscaling, es necesario obtener una estimaci√≥n precisa del consumo de CPU por usuario virtual (VU) o por request. Para ello, se debe realizar primero el experimento descrito en el archivo [`02_exp0_microbenchmark.md`](./02_exp0_microbenchmark.md).

Dicho experimento proporciona los valores de referencia necesarios para estimar la demanda te√≥rica de CPU a partir de la carga generada. Estos valores deben ser actualizados manualmente en los scripts de an√°lisis que construyen las curvas de elasticidad y calculan las m√©tricas.

### üìå Variables a definir (extra√≠das del microbenchmark):

```python
# ==============================================================================
# IMPORTANTE: AJUSTAR SEG√öN MICROBENCHMARK
# ==============================================================================
cpu_per_vu = 1.50    # millicores por VU
cpu_per_req = 0.05   # millicores por request
requests_per_vu_per_second = 1  # Asumido por dise√±o del benchmark (1 request/seg por VU)
```

Estas variables permiten traducir la carga generada (VUs o requests por segundo) en demanda estimada de CPU (millicores), que luego se compara con la oferta disponible para construir las curvas de elasticidad.

### üõ†Ô∏è Archivos donde deben ser modificadas:

- `analysis/calculate_elasticity_metrics.py`
- `analysis/plot_elasticity_curve.py`
- `analysis/plot_elasticity_curve_with_events.py`

> ‚ö†Ô∏è Si no se actualizan estos valores, los resultados obtenidos del an√°lisis de elasticidad ser√°n inconsistentes con la carga real aplicada al sistema.


---

## üìä Resultados obtenidos

El an√°lisis posterior al experimento produce una serie de salidas en formato gr√°fico y archivos de texto que resumen el comportamiento del sistema, la respuesta del HPA, y la calidad del escalamiento.

### üìà Gr√°ficos generados

Los gr√°ficos se encuentran organizados en subcarpetas dentro de `analysis/images/` seg√∫n el tipo de informaci√≥n visualizada:

#### üß† Uso de CPU por pod (`analysis/images/cpu_pod/`)
- `cpu_usage_per_pod.png`: Uso de CPU de todos los pods a lo largo del tiempo.
- `cpu_usage_per_pod_with_events.png`: Igual al anterior, pero incluye l√≠neas verticales que indican eventos de escalamiento.
- `podX_cpu.png`: Uso de CPU individual por pod (siendo X el n√∫mero del pod correspondiente).
- `podX_cpu_with_events.png`: Versi√≥n con eventos (siendo X el n√∫mero del pod correspondiente).

#### üî¢ Conteo de pods (`analysis/images/pod_count/`)
- `pod_count_over_time.png`: Evoluci√≥n del n√∫mero de r√©plicas durante la prueba.
- `pod_count_over_time_with_events.png`: Incluye momentos de escalamiento.

#### ‚öñÔ∏è Curvas de elasticidad (`analysis/images/elasticity/`)
- `elasticity_curve_vu.png`: Comparaci√≥n entre demanda estimada (por VU) y oferta de CPU.
- `elasticity_curve_vus_with_events.png`: Versi√≥n con eventos.
- `elasticity_curve_req.png`: Curva basada en estimaci√≥n por n√∫mero de requests.
- `elasticity_curve_reqs_with_events.png`: Versi√≥n con eventos.

#### üìâ M√©tricas indirectas (`analysis/images/indirect_metrics/`)
- `latency_avg.png`, `latency_avg_events.png`: Latencia promedio.
- `throughput.png`, `throughput_events.png`: Rendimiento total (requests por segundo).
- `throughput_vs_vus.png`: Relaci√≥n entre carga y throughput.
- `http_errors.png`, `http_errors_events.png`: Errores HTTP detectados.

### üìÑ Archivos de salida (en `output/`)

Estos archivos contienen los datos crudos y procesados del experimento:

- `basic_metrics.csv`: M√©tricas de CPU y r√©plicas recolectadas durante la prueba.
- `scaling_events.csv`: Eventos de escalamiento sin procesar.
- `scaling_events_clean.csv`: Eventos sincronizados con el inicio del test.
- `k6_summary.json`: Resumen de la ejecuci√≥n de carga desde k6.
- `k6_results.csv`: Resultados detallados por segundo desde k6.
- `k6_start_time.txt`: Marca de tiempo del inicio de la carga.
- `elasticity_metrics_vus.txt`: M√©tricas de elasticidad (por VU), incluyendo precisi√≥n, tiempos de sub/sobreaprovisionamiento y recursos usados/desperdiciados.
- `elasticity_metrics_requests.txt`: Misma informaci√≥n, calculada en funci√≥n de la demanda por n√∫mero de requests.

La interpretaci√≥n combinada de estas m√©tricas y gr√°ficos permite tener una visi√≥n detallada de la eficiencia y elasticidad del sistema bajo la configuraci√≥n b√°sica del HPA.

---

## üß™ Ejecuci√≥n y automatizaci√≥n del proceso

Este experimento puede ejecutarse manualmente paso a paso o de forma automatizada mediante un script Bash. A continuaci√≥n se describe cada enfoque.

---

### üßæ Ejecuci√≥n manual paso a paso

Para ejecutar el experimento manualmente, se deben realizar las siguientes acciones desde la ra√≠z del proyecto (`elasticity-m1`):

1. **Aplicar los manifiestos de Kubernetes**:

```bash
kubectl apply -f basic-autoscaling/manifests/nginx-deployment.yaml
kubectl apply -f basic-autoscaling/manifests/hpa.yaml
```

2. **Esperar que los pods est√©n listos**, y obtener la IP del nodo para configurar la prueba:

```bash
kubectl get nodes -o wide
```

3. **Editar el script de carga** para reemplazar `<IP_DEL_CLUSTER>` por la IP del nodo en:
```
basic-autoscaling/scripts/basic_load_test.js
```

4. **Iniciar los recolectores en segundo plano**:

```bash
bash basic-autoscaling/scripts/metric_collector_basic.sh &
METRIC_PID=$!
bash basic-autoscaling/scripts/capture_deployment_events.sh &
EVENTS_PID=$!
```

5. **Ejecutar la prueba de carga con k6**:

```bash
k6 run --out csv=basic-autoscaling/output/k6_results.csv \
       --summary-export=basic-autoscaling/output/k6_summary.json \
       basic-autoscaling/scripts/basic_load_test.js
```

6. **Esperar unos segundos adicionales** tras la prueba:

```bash
sleep 30
```

7. **Detener los recolectores**:

```bash
kill $METRIC_PID
kill $EVENTS_PID
```

8. **Ejecutar an√°lisis desde Docker**:

```bash
docker build -t basic-autoscaling-analysis basic-autoscaling/analysis
docker run --rm \
  -v "$(pwd)/basic-autoscaling/output:/app/output" \
  -v "$(pwd)/basic-autoscaling/analysis/images:/app/images" \
  basic-autoscaling-analysis
```

9. **Eliminar recursos de Kubernetes**:

```bash
kubectl delete -f basic-autoscaling/manifests/nginx-deployment.yaml
kubectl delete -f basic-autoscaling/manifests/hpa.yaml
```

---

### ü§ñ Ejecuci√≥n automatizada con `exp1_basic-autoscaling.sh`

Todo el flujo anterior ha sido automatizado mediante el script:

```
exp1_basic-autoscaling.sh
```

Este script ejecuta cada uno de los pasos en orden, incluyendo despliegue, recolecci√≥n de datos, ejecuci√≥n de carga, an√°lisis y limpieza de recursos.

> üìÇ El script se encuentra en la ra√≠z del proyecto:  
> `elasticity-m1/exp1_basic-autoscaling.sh`

#### ‚ñ∂Ô∏è Modo de ejecuci√≥n:

```bash
bash exp1_basic-autoscaling.sh
```

> ‚ö†Ô∏è **Importante**: el script debe ejecutarse desde la ra√≠z del repositorio (`elasticity-m1`) para que las rutas relativas funcionen correctamente.

#### üîç Acciones que realiza autom√°ticamente:

1. Aplica los manifiestos de Kubernetes.
2. Muestra la IP del cl√∫ster y solicita al usuario editar la IP en el archivo `basic_load_test.js`.
3. Inicia los recolectores de m√©tricas y eventos.
4. Ejecuta la prueba de carga con k6.
5. Espera un breve periodo tras la carga.
6. Detiene los procesos de recolecci√≥n.
7. Ejecuta el an√°lisis desde Docker.
8. Elimina los recursos desplegados en el cl√∫ster.

Este enfoque garantiza reproducibilidad, automatizaci√≥n y una m√≠nima intervenci√≥n manual.




# ğŸ§ª 02 - Microbenchmark de Elasticidad: NGINX

Este documento describe el experimento de microbenchmark realizado para estimar el uso de CPU por solicitud y por usuario virtual en un microservicio NGINX. Este paso es fundamental para calcular la *resource demand* en pruebas posteriores de elasticidad.

---

## ğŸŒŸ Objetivo del experimento

- Obtener una referencia del consumo real de CPU en condiciones controladas.
- Estimar:
  - Millicores utilizados por *request*.
  - Millicores utilizados por *usuario virtual* (VU).

Esta informaciÃ³n se usarÃ¡ como insumo para construir la curva de demanda esperada en futuros experimentos.

---

## âš™ï¸ Arquitectura del experimento

- **Microservicio**: NGINX
- **Cluster**: Kubernetes local (usando HPA)
- **Generador de carga**: k6
- **MÃ©tricas recolectadas**: CPU por pod, nÃºmero de pods
- **DuraciÃ³n de la prueba**: 1 minuto de carga constante (10 VUs)

---

## â™»ï¸ Flujo del experimento

| Paso | DescripciÃ³n |
|------|-------------|
| 1. | Despliegue de NGINX y HPA en Kubernetes |
| 2. | IniciaciÃ³n de script de recolecciÃ³n de mÃ©tricas |
| 3. | EjecuciÃ³n de prueba con k6 (`benchmark_test.js`) |
| 4. | FinalizaciÃ³n de captura de mÃ©tricas |
| 5. | AnÃ¡lisis automÃ¡tico con Python dentro de Docker |

---

## ğŸ“¦ Configuraciones utilizadas

### Manifiestos Kubernetes (ubicados en `files/microbenchmark/manifests/`):

- `deployment.yaml`: Despliegue del pod NGINX con requests y limits definidos, necesario para que el HPA pueda actuar correctamente.
- `hpa.yaml`: Define un autoscaler con mÃ©trica basada en uso promedio de CPU (25%) y lÃ­mites de rÃ©plicas.

### Script de carga (`benchmark_test.js`):
Ubicado en `files/microbenchmark/loadtest/`, genera carga constante:
```js
stages: [
  { duration: '1m', target: 10 },
]
```
Esto simula 10 usuarios virtuales haciendo peticiones continuas durante 1 minuto.

### RecolecciÃ³n de mÃ©tricas:
Script: `metric_collector_microbenchmark.sh`
Ubicado en: `files/microbenchmark/scripts/`

RecolecciÃ³n cada 10 segundos de:
- CPU por pod (millicores)
- Uso relativo de CPU (%)
- NÃºmero de pods activos

---

## ğŸ”¢ CÃ¡lculo de estimaciones

El script `analyze_microbenchmark.py` fusiona las mÃ©tricas de k6 (`k6_summary.json`) con las mÃ©tricas de Kubernetes (`microbenchmark_metrics.csv`) y calcula:

- CPU total utilizada:
```python
cpu_total_millicores = df["cpu(millicores)"].sum()
```
- CPU por request:
```python
cpu_per_request = cpu_total_millicores / total_requests
```
- CPU por VU:
```python
cpu_per_vu = cpu_total_millicores / vus_max
```

Estos valores se guardan en:
- Texto: `files/microbenchmark_summary.txt`
- CSV estructurado: `files/microbenchmark_summary.csv`

---

## ğŸ“Š Resultados obtenidos

### Resumen generado por `analyze_microbenchmark.py`

| MÃ©trica                | Valor                        |
|------------------------|------------------------------|
| Total de requests      | 303                          |
| MÃ¡ximo de VUs          | 10                           |
| CPU total utilizada    | 15.00 millicores             |
| CPU por request        | 0.05 millicores/request      |
| CPU por VU             | 1.50 millicores/VU           |

### GrÃ¡ficos generados

ğŸ“ `images/cpu_usage_per_pod.png`

Este grÃ¡fico muestra la evoluciÃ³n del uso relativo de CPU (%) para el pod activo durante el experimento. Se observa un patrÃ³n de consumo suave y sostenido, lo que respalda la estabilidad del microservicio:

![CPU por pod](images/cpu_usage_per_pod.png)

ğŸ“ `images/pod_count_over_time.png`

Este grÃ¡fico refleja que durante toda la prueba se mantuvo activo un solo pod. Esto era esperable ya que el uso de CPU nunca superÃ³ el umbral del HPA:

![EvoluciÃ³n de pods](images/pod_count_over_time.png)

---

## ğŸ“ Estructura del experimento

```
files/microbenchmark/
â”œâ”€â”€ manifests/                            # Archivos YAML para despliegue
â”‚   â”œâ”€â”€ deployment.yaml                   # Deployment del pod nginx con requests/limits
â”‚   â””â”€â”€ hpa.yaml                          # Autoscaler HPA con CPU objetivo = 25%
â”œâ”€â”€ loadtest/
â”‚   â””â”€â”€ benchmark_test.js                # Script k6 para generar carga controlada
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ metric_collector_microbenchmark.sh # Recolector de CPU y n. de pods
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ microbenchmark_metrics.csv        # MÃ©tricas recolectadas por el script
â”‚   â””â”€â”€ k6_summary.json                   # Resumen de carga generado por k6
â”œâ”€â”€ files/
â”‚   â”œâ”€â”€ microbenchmark_summary.txt        # Resultados legibles por humanos
â”‚   â””â”€â”€ microbenchmark_summary.csv        # Resultados estructurados en una fila CSV
â””â”€â”€ analysis/
    â”œâ”€â”€ plot_cpu_usage.py                 # Genera grÃ¡fico de uso de CPU por pod
    â”œâ”€â”€ plot_pod_count.py                 # GrÃ¡fico del n. de pods activos
    â”œâ”€â”€ analyze_microbenchmark.py         # Calcula CPU/request y CPU/VU
    â”œâ”€â”€ requirements.txt                  # Dependencias Python
    â””â”€â”€ Dockerfile                        # Contenedor para ejecutar anÃ¡lisis
```

---

## âœ… Conclusiones

A partir del microbenchmark realizado se obtuvieron los siguientes valores clave:

| MÃ©trica                  | Valor                         |
|--------------------------|-------------------------------|
| CPU por solicitud        | 0.05 millicores/request       |
| CPU por usuario virtual  | 1.50 millicores/VU            |
| NÃºmero de pods activos   | 1 (sin escalar)               |
| Consumo total observado  | 15 millicores                 |

Estos valores servirÃ¡n como base para definir la demanda estimada en futuros experimentos de elasticidad. La prueba tambiÃ©n valida el funcionamiento del sistema de recolecciÃ³n y anÃ¡lisis automatizado.

---

## ğŸ¤– AutomatizaciÃ³n del proceso

Todo el proceso descrito en este documento puede ejecutarse automÃ¡ticamente mediante el script `exp0_microbenchmark.sh`, ubicado en la carpeta `files/`.

### ğŸ“ UbicaciÃ³n
```
files/exp0_microbenchmark.sh
```

### â–¶ï¸ EjecuciÃ³n
Este script debe ejecutarse desde dentro de la carpeta `files/`:
```bash
cd files/
bash exp0_microbenchmark.sh
```

### ğŸ”„ QuÃ© realiza automÃ¡ticamente

1. Aplica los manifiestos en Kubernetes:
```bash
kubectl apply -f microbenchmark/manifests/deployment.yaml
kubectl apply -f microbenchmark/manifests/hpa.yaml
```

2. Solicita al usuario actualizar la IP del clÃºster en el script de k6.

3. Lanza el recolector de mÃ©tricas en segundo plano:
```bash
bash microbenchmark/scripts/metric_collector_microbenchmark.sh &
```

4. Ejecuta la prueba con k6 y exporta resumen:
```bash
k6 run --summary-export output/k6_summary.json microbenchmark/loadtest/benchmark_test.js
```

5. Espera y luego finaliza el recolector:
```bash
kill $METRIC_PID
```

6. Construye y ejecuta el contenedor de anÃ¡lisis:
```bash
docker build -t microbenchmark-analysis microbenchmark/analysis

docker run --rm \
  -v "$(pwd)/microbenchmark/output:/app/output" \
  -v "$(pwd)/microbenchmark/analysis/files:/app/files" \
  -v "$(pwd)/microbenchmark/analysis/images:/app/images" \
  microbenchmark-analysis
```

7. Limpia los recursos de Kubernetes:
```bash
kubectl delete -f microbenchmark/manifests/
```

Este enfoque garantiza reproducibilidad, facilita la recolecciÃ³n de datos y reduce errores humanos durante la ejecuciÃ³n del experimento.


# ğŸ§ª Microbenchmark de Elasticidad: NGINX

Este documento describe el experimento de microbenchmark realizado para estimar el consumo de CPU por solicitud y por usuario virtual (VU) en un microservicio NGINX. Este experimento constituye un **paso previo esencial** para estructurar correctamente los anÃ¡lisis de elasticidad en escenarios mÃ¡s complejos.

La estimaciÃ³n de consumo de recursos bajo carga controlada permite convertir mÃ©tricas de carga abstractas (como el nÃºmero de usuarios virtuales o el nÃºmero de solicitudes) en una estimaciÃ³n directa de **demanda de CPU en millicores**. Esta conversiÃ³n es crÃ­tica para construir curvas de demanda esperada y, con base en ellas, poder comparar la oferta real del sistema durante los experimentos de elasticidad.

Se eligiÃ³ NGINX como microservicio base por su **simplicidad, predictibilidad y baja latencia**, lo que permite centrarse en la dinÃ¡mica de consumo de recursos sin introducir variabilidad innecesaria.

El microbenchmark se ejecuta en el **mismo entorno de Kubernetes local** que se utiliza en los experimentos principales, incluyendo la presencia de un HPA (Horizontal Pod Autoscaler), lo que garantiza que los resultados obtenidos sean completamente consistentes con el resto del sistema.

---

## ğŸ¯ Objetivo del experimento

El propÃ³sito principal es estimar cuÃ¡ntos **millicores de CPU** consume el sistema bajo condiciones de carga controlada. En particular, se busca obtener:

- ğŸ§® **Millicores por request**
- ğŸ‘¤ **Millicores por usuario virtual (VU)**

Estos valores servirÃ¡n como entrada directa para calcular la demanda de recursos (*resource demand*) en los experimentos de elasticidad, y son clave para habilitar el cÃ¡lculo de mÃ©tricas como:

- PrecisiÃ³n de escalado
- Tiempos acumulados en sub/sobreaprovisionamiento
- MÃ©tricas de elasticidad hacia arriba, hacia abajo y global

---

## ğŸ§© DescripciÃ³n general del sistema

Este microbenchmark fue diseÃ±ado para ejecutarse en el mismo entorno de pruebas que los experimentos de elasticidad: un **cluster de Kubernetes local** con soporte para escalamiento automÃ¡tico mediante **Horizontal Pod Autoscaler (HPA)**. Aunque en esta prueba el escalamiento no se activa (por el bajo nivel de carga), el HPA permanece activo para mantener coherencia con el resto del sistema.

El flujo de software se basa en una arquitectura simple pero representativa:

| Componente         | Rol en el experimento                              |
|--------------------|----------------------------------------------------|
| **NGINX**          | Microservicio bajo prueba                          |
| **K6**             | Generador de carga controlada                      |
| **Kubernetes**     | Plataforma de orquestaciÃ³n                         |
| **HPA**            | Escalador automÃ¡tico (activo pero sin efecto aquÃ­) |
| **Scripts Bash**   | AutomatizaciÃ³n de recolecciÃ³n y anÃ¡lisis           |
| **Python + Docker**| AnÃ¡lisis de resultados                             |

### ğŸ”¹ Condiciones experimentales

- Carga constante: **10 usuarios virtuales (VUs)** durante **1 minuto**
- No se espera escalamiento: el uso de CPU no deberÃ­a superar el umbral del HPA
- ObservaciÃ³n principal: consumo de CPU y nÃºmero de pods durante la prueba
- RecolecciÃ³n periÃ³dica de mÃ©tricas: cada **10 segundos**

Esta configuraciÃ³n permite capturar de manera precisa la relaciÃ³n entre la carga generada y los recursos consumidos, bajo un entorno idÃ©ntico al que se utilizarÃ¡ en los anÃ¡lisis de elasticidad posteriores.

---

## ğŸ”„ Flujo del experimento

El microbenchmark sigue una secuencia clara y automatizada de pasos, cuyo objetivo es generar carga controlada, recolectar mÃ©tricas relevantes y procesarlas para obtener estimaciones clave.

| Paso | DescripciÃ³n tÃ©cnica |
|------|---------------------|
| 1ï¸âƒ£   | **Despliegue del microservicio**: se aplican los manifiestos de Kubernetes (`deployment.yaml` y `hpa.yaml`) para iniciar el pod de NGINX junto con su autoscaler. |
| 2ï¸âƒ£   | **Inicio del recolector de mÃ©tricas**: un script en Bash ejecuta la captura periÃ³dica del uso de CPU por pod y el nÃºmero de rÃ©plicas activas, guardando esta informaciÃ³n en CSV. |
| 3ï¸âƒ£   | **EjecuciÃ³n de la carga**: se lanza el script `benchmark_test.js` con K6, simulando 10 usuarios virtuales generando solicitudes de forma constante durante 1 minuto. |
| 4ï¸âƒ£   | **FinalizaciÃ³n de la recolecciÃ³n**: el script de captura se detiene despuÃ©s de la prueba, asegurando que se hayan registrado todas las mÃ©tricas necesarias. |
| 5ï¸âƒ£   | **AnÃ¡lisis de resultados**: un contenedor Docker ejecuta el script `analyze_microbenchmark.py`, que combina las mÃ©tricas de uso de CPU con los resultados de K6 para calcular las estimaciones finales. |

Este flujo completo puede ejecutarse de forma automÃ¡tica mediante el script `exp0_microbenchmark.sh`, lo que asegura reproducibilidad y facilita futuras repeticiones del experimento.

---

## âš™ï¸ Configuraciones utilizadas

A continuaciÃ³n se detallan las configuraciones clave utilizadas durante el experimento. Todas ellas estÃ¡n contenidas en la carpeta `files/microbenchmark/`.

---

### ğŸ“¦ Manifiestos de Kubernetes

UbicaciÃ³n: `files/microbenchmark/manifests/`

- **`deployment.yaml`**: Define el despliegue de un pod con NGINX. Establece los `requests` y `limits` de CPU requeridos por el HPA.
- **`hpa.yaml`**: Configura un Horizontal Pod Autoscaler con umbral de CPU objetivo del 25% y un rango de entre 1 y 10 rÃ©plicas. Aunque no se espera escalamiento en esta prueba, se mantiene activo por coherencia con el resto del sistema.

---

### ğŸ§ª Script de carga (K6)

UbicaciÃ³n: `files/microbenchmark/loadtest/benchmark_test.js`

Genera una carga constante de 10 usuarios virtuales durante 1 minuto. Este patrÃ³n permite observar el comportamiento del consumo de recursos bajo condiciones estables, sin variaciones bruscas de trÃ¡fico.

---

### ğŸ§¾ RecolecciÃ³n de mÃ©tricas

UbicaciÃ³n: `files/microbenchmark/scripts/metric_collector_microbenchmark.sh`

Este script captura mÃ©tricas cada 10 segundos durante la prueba, incluyendo:

- Uso de CPU por pod (en millicores)
- Uso relativo de CPU (en porcentaje)
- NÃºmero de pods activos

Los datos recolectados se almacenan en un archivo CSV (`microbenchmark_metrics.csv`) para su posterior anÃ¡lisis.

---

### ğŸ“Š Scripts de anÃ¡lisis y visualizaciÃ³n (Python)

UbicaciÃ³n: `files/microbenchmark/analysis/`

DespuÃ©s de la ejecuciÃ³n de la prueba, los datos recolectados son procesados automÃ¡ticamente por una serie de scripts en Python contenidos en esta carpeta. Su propÃ³sito es calcular mÃ©tricas clave y generar visualizaciones Ãºtiles para evaluar el consumo de recursos.

- **`analyze_microbenchmark.py`**  
  Script principal de anÃ¡lisis. Lee el resumen de carga (`k6_summary.json`) y las mÃ©tricas del sistema (`microbenchmark_metrics.csv`) para calcular:
  - CPU total utilizada (millicores)
  - CPU por request
  - CPU por VU  
  Los resultados se exportan como archivos de texto (`microbenchmark_summary.txt`) y CSV (`microbenchmark_summary.csv`).

- **`plot_cpu_usage.py`**  
  Genera una grÃ¡fica temporal del uso de CPU (en porcentaje) para los pods activos. Permite observar la estabilidad y carga efectiva del sistema.

- **`plot_pod_count.py`**  
  Produce una grÃ¡fica del nÃºmero de pods activos a lo largo del tiempo. Verifica que no haya habido escalamiento accidental durante la prueba.

- **`requirements.txt`**  
  Lista de dependencias necesarias para ejecutar los scripts (`pandas`, `matplotlib`, entre otras).

- **`Dockerfile`**  
  Define la imagen Docker utilizada para ejecutar todo el anÃ¡lisis en un entorno limpio y reproducible. Esto elimina la necesidad de instalar dependencias localmente y asegura que los resultados sean consistentes.

---

Estas herramientas complementan el entorno de ejecuciÃ³n, permitiendo transformar datos crudos en mÃ©tricas cuantificables y visualizaciones interpretables, todo de forma automÃ¡tica.

---

## ğŸ”¢ CÃ¡lculo de estimaciones

El objetivo principal del microbenchmark es obtener estimaciones confiables sobre el consumo de CPU por unidad de carga. Estas estimaciones permiten transformar medidas abstractas como "cantidad de usuarios virtuales" o "nÃºmero de solicitudes" en una **demanda de recursos cuantificable en millicores**, lo que resulta indispensable para construir curvas de elasticidad y evaluar el desempeÃ±o del autoscaler.

El proceso de cÃ¡lculo estÃ¡ totalmente automatizado mediante el script `analyze_microbenchmark.py`, que integra las mÃ©tricas recolectadas desde Kubernetes y los resultados de carga generados por K6.

---

### ğŸ“Œ Fuentes de datos utilizadas

- **`microbenchmark_metrics.csv`**  
  Contiene muestras del uso de CPU (en millicores) y el nÃºmero de pods activos recolectadas cada 10 segundos durante el experimento.

- **`k6_summary.json`**  
  Archivo JSON generado automÃ¡ticamente por K6, donde se registran:
  - El nÃºmero total de solicitudes procesadas (`requests`)
  - El nÃºmero mÃ¡ximo de usuarios virtuales (`vus_max`)
  - La duraciÃ³n total de la prueba

---

### ğŸ“ Estimaciones calculadas

A partir de estas dos fuentes, se derivan las siguientes mÃ©tricas clave:

#### 1. **CPU total utilizada**

Esta mÃ©trica representa la cantidad total de recursos de CPU consumidos (en millicores) durante todo el experimento. Se obtiene a partir del archivo `microbenchmark_metrics.csv`, donde se recolecta el uso de CPU por pod cada 10 segundos.

Para calcular esta mÃ©trica, se suman todos los valores registrados en la columna `cpu(millicores)`:

CÃ³digo relevante del script `analyze_microbenchmark.py`:
```python
cpu_total_millicores = df["cpu(millicores)"].sum()
```

#### 2. **CPU por solicitud (millicores/request)**

Esta mÃ©trica estima cuÃ¡ntos millicores de CPU se consumen, en promedio, por cada solicitud HTTP enviada al sistema. Es fundamental para generar curvas de demanda basadas en trÃ¡fico.

El valor del nÃºmero total de solicitudes (`total_requests`) se obtiene del archivo `k6_summary.json`, generado por la herramienta K6 al finalizar la prueba:
```python
total_requests = k6_data["metrics"]["http_reqs"]["count"]
```

CÃ¡lculo:
```python
cpu_per_request = cpu_total_millicores / total_requests if total_requests else 0
```

Esta estimaciÃ³n es Ãºtil para construir una curva de demanda basada en el volumen de trÃ¡fico (requests por segundo), independientemente del nÃºmero de usuarios virtuales.

#### 3. **CPU por usuario virtual (millicores/VU)**

Esta mÃ©trica estima el consumo promedio de CPU por cada usuario virtual activo durante la prueba. Es Ãºtil cuando se modela la carga en tÃ©rminos de usuarios concurrentes.

El nÃºmero mÃ¡ximo de usuarios virtuales (`vus_max`) tambiÃ©n se extrae desde `k6_summary.json`:
```python
vus_max = int(k6_data["metrics"]["vus_max"]["value"])
```

CÃ¡lculo:
```python
cpu_per_vu = cpu_total_millicores / vus_max if vus_max else 0
```

Resulta Ãºtil cuando la carga del sistema se modela en tÃ©rminos de usuarios concurrentes, como suele hacerse en herramientas de pruebas de carga o simulaciones de comportamiento de usuarios.

---

### ğŸ“ ExportaciÃ³n de resultados

Una vez calculadas, las estimaciones son exportadas en los siguientes formatos:

- **`microbenchmark_summary.txt`**: resumen legible para revisiÃ³n manual o presentaciÃ³n
- **`microbenchmark_summary.csv`**: archivo estructurado en formato tabular para uso automatizado en experimentos posteriores

Estas salidas sirven como **insumo directo** para calcular la *demanda de recursos* en experimentos de elasticidad, permitiendo generar curvas teÃ³ricas de demanda que luego se comparan con la oferta observada por el sistema en producciÃ³n.

---

### ğŸ’¡ Relevancia del cÃ¡lculo

Este proceso es fundamental porque permite estimar, de forma cuantificable, la demanda de CPU asociada a una carga determinada. Sin estas mÃ©tricas, no serÃ­a posible transformar el comportamiento observado del sistema (usuarios o requests) en una estimaciÃ³n concreta del consumo de recursos. Estas estimaciones son el insumo base para evaluar si el sistema estÃ¡ correctamente aprovisionado en pruebas de elasticidad.


---

## ğŸ“Š Resultados obtenidos

Tras ejecutar el microbenchmark sobre el microservicio NGINX, se obtuvieron los siguientes valores a partir del anÃ¡lisis automÃ¡tico realizado por el script `analyze_microbenchmark.py`.

> âš ï¸ **Importante:** Estos valores corresponden a una ejecuciÃ³n especÃ­fica sobre un microservicio concreto (NGINX) bajo carga controlada. Los resultados pueden variar significativamente dependiendo del tipo de aplicaciÃ³n, su comportamiento interno, la configuraciÃ³n de recursos y el entorno de despliegue. Por lo tanto, se recomienda repetir este microbenchmark para cada microservicio que se desee evaluar.

---

### ğŸ“ˆ Resumen numÃ©rico

| MÃ©trica                     | Valor                          | InterpretaciÃ³n |
|----------------------------|--------------------------------|----------------|
| Total de requests          | 303 solicitudes                | Cantidad total de solicitudes procesadas durante el experimento. |
| MÃ¡ximo de usuarios virtuales (VU) | 10                            | NÃºmero de VUs activos generando carga durante toda la prueba. |
| CPU total utilizada        | 15.00 millicores               | Suma del uso de CPU reportado por Kubernetes a lo largo de la prueba. |
| CPU por solicitud          | 0.05 millicores/request        | Promedio de millicores necesarios para atender una solicitud individual. |
| CPU por usuario virtual    | 1.50 millicores/VU             | Promedio de millicores consumidos por cada VU activo. |

Estas mÃ©tricas permiten cuantificar la *demanda de recursos* del microservicio y son fundamentales para modelar correctamente escenarios de elasticidad.

---

### ğŸ“Š GrÃ¡ficas generadas

A continuaciÃ³n se analizan las grÃ¡ficas generadas durante la ejecuciÃ³n:

---

#### ğŸ”¹ NÃºmero de pods activos

![EvoluciÃ³n de pods](images/microbenchmark/pod_count_over_time.png)

Este grÃ¡fico muestra que durante toda la duraciÃ³n del experimento el nÃºmero de pods activos fue constante e igual a **1**. Esto era esperable, ya que:

- El HPA estaba configurado pero no se alcanzÃ³ el umbral de uso de CPU (25%) que desencadenarÃ­a el escalado.
- La carga generada fue deliberadamente baja (10 VUs) para mantener el sistema estable.

**InterpretaciÃ³n:**  
La ausencia de escalamiento confirma que el sistema operÃ³ en un rango de carga estable y bajo aprovisionamiento controlado, ideal para extraer mediciones precisas sin interferencias.

---

#### ğŸ”¹ Uso relativo de CPU (%)

![Uso de CPU por pod](images/microbenchmark/cpu_usage_per_pod.png)

Este grÃ¡fico muestra el porcentaje de uso de CPU del pod de NGINX a lo largo del tiempo.

- Se observa un aumento progresivo en el consumo, estabilizÃ¡ndose alrededor del **3% de uso** en la segunda mitad de la prueba.
- La caÃ­da al final corresponde al tÃ©rmino de la carga generada.

**InterpretaciÃ³n:**  
El consumo gradual de CPU refleja un comportamiento estable y predecible del microservicio. La baja utilizaciÃ³n confirma que la carga generada no exigiÃ³ el escalamiento, pero sÃ­ permitiÃ³ detectar consumo real, suficiente para derivar las mÃ©tricas de demanda.

---

Los resultados numÃ©ricos y visuales muestran que el sistema respondiÃ³ de manera estable y controlada, lo que valida la calidad del experimento. Las mÃ©tricas obtenidas permitirÃ¡n construir curvas de demanda realistas en futuras pruebas de elasticidad y calcular indicadores como la precisiÃ³n del escalado y los tiempos de sobre/subaprovisionamiento.

---

## ğŸ“ Estructura del experimento

A continuaciÃ³n se muestra la organizaciÃ³n del experimento dentro del directorio `files/microbenchmark/`. Esta estructura agrupa todos los componentes necesarios para ejecutar, monitorear y analizar el microbenchmark de forma modular y ordenada.

```
files/microbenchmark/
â”œâ”€â”€ manifests/                            # Archivos YAML para despliegue
â”‚   â”œâ”€â”€ deployment.yaml                   # Deployment del pod nginx con requests/limits
â”‚   â””â”€â”€ hpa.yaml                          # Autoscaler HPA con CPU objetivo = 25%
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ metric_collector_microbenchmark.sh # Recolector de CPU y n. de pods
â”‚   â””â”€â”€ benchmark_test.js                # Script k6 para generar carga controlada
â”œâ”€â”€ files/        
â”‚   â””â”€â”€                                     
â””â”€â”€ analysis/
    â”œâ”€â”€ files/
    â”‚   â”œâ”€â”€ microbenchmark_metrics.csv        
    â”‚   â”œâ”€â”€ microbenchmark_summary.txt        
    â”œâ”€â”€ images/
    â”‚   â”œâ”€â”€ cpu_pod/
    â”‚   â”‚   â””â”€â”€ pod1_cpu.png
    â”‚   â”œâ”€â”€ cpu_usage_per_pod.png           # Grafica sobre el uso de CPU por pod 
    â”‚   â””â”€â”€ pod_count_over_time.png        # Grafica de pods a traves del tiempo
    â”œâ”€â”€ plot_cpu_usage.py                 # Genera grÃ¡fico de uso de CPU por pod
    â”œâ”€â”€ plot_pod_count.py                 # GrÃ¡fico del n. de pods activos
    â”œâ”€â”€ analyze_microbenchmark.py         # Calcula CPU/request y CPU/VU
    â”œâ”€â”€ requirements.txt                  # Dependencias Python
    â””â”€â”€ Dockerfile                        # Contenedor para ejecutar anÃ¡lisis
```

Esta organizaciÃ³n permite:
- Acceder de forma ordenada a mÃ©tricas, resultados y visualizaciones
- Facilitar el mantenimiento y reutilizaciÃ³n del experimento para otros microservicios

---

## ğŸ¤– AutomatizaciÃ³n del proceso

La ejecuciÃ³n del microbenchmark puede realizarse de forma manual o completamente automatizada mediante un script Bash. A continuaciÃ³n se presentan ambas alternativas.

---

### ğŸ§­ EjecuciÃ³n manual paso a paso

Si se desea ejecutar el microbenchmark sin utilizar automatizaciÃ³n, los siguientes comandos deben ejecutarse en orden para replicar el flujo completo:

1. **Aplicar manifiestos en Kubernetes:**
   ```bash
   kubectl apply -f microbenchmark/manifests/deployment.yaml
   kubectl apply -f microbenchmark/manifests/hpa.yaml
   ```
   Esto despliega el pod NGINX con configuraciÃ³n de CPU y activa el HPA.

2. **Esperar a que el pod se inicialice correctamente:**
   ```bash
   sleep 20
   ```

3. **Obtener la IP del clÃºster y actualizar el script de carga:**
   ```bash
   kubectl get nodes -o wide
   # Editar manualmente 'benchmark_test.js' y reemplazar <IP_DEL_CLUSTER>
   ```

4. **Iniciar el recolector de mÃ©tricas en segundo plano:**
   ```bash
   bash microbenchmark/scripts/metric_collector_microbenchmark.sh &
   METRIC_PID=$!
   ```

5. **Ejecutar la prueba con K6:**
   ```bash
   k6 run --summary-export microbenchmark/output/k6_summary.json \
     microbenchmark/scripts/benchmark_test.js
   ```

6. **Esperar para capturar la estabilizaciÃ³n post-carga:**
   ```bash
   sleep 30
   ```

7. **Finalizar el recolector de mÃ©tricas:**
   ```bash
   kill $METRIC_PID
   ```

8. **Ejecutar anÃ¡lisis automÃ¡tico con Docker:**
   ```bash
   docker build -t microbenchmark-analysis microbenchmark/analysis
   docker run --rm \
     -v "$(pwd)/microbenchmark/output:/app/output" \
     -v "$(pwd)/microbenchmark/analysis/files:/app/files" \
     -v "$(pwd)/microbenchmark/analysis/images:/app/images" \
     microbenchmark-analysis
   ```

9. **Eliminar los recursos desplegados en Kubernetes:**
   ```bash
   kubectl delete -f microbenchmark/manifests/deployment.yaml
   kubectl delete -f microbenchmark/manifests/hpa.yaml
   ```

---

### ğŸ§ª EjecuciÃ³n automÃ¡tica con `exp0_microbenchmark.sh`

Todo el flujo descrito anteriormente ha sido encapsulado en el script `exp0_microbenchmark.sh`, ubicado en la raÃ­z del directorio `files/`.

Este script realiza las siguientes acciones:

1. Despliega NGINX y su HPA en Kubernetes.
2. Solicita al usuario que actualice la IP en el script de carga.
3. Inicia el recolector de mÃ©tricas.
4. Ejecuta la prueba con K6.
5. Detiene el recolector.
6. Ejecuta el anÃ¡lisis completo en Docker.
7. Limpia los recursos del clÃºster.

#### ğŸ“ UbicaciÃ³n del script

```
files/exp0_microbenchmark.sh
```

#### â–¶ï¸ Instrucciones de uso

> âš ï¸ **Requisito importante:** el script debe ejecutarse desde la raÃ­z del proyecto (`elasticity-m1`) para que las rutas relativas funcionen correctamente.

```bash
cd elasticity-m1/files
bash exp0_microbenchmark.sh
```

Al finalizar, el script imprime la ubicaciÃ³n de todos los archivos generados:

- CSV de mÃ©tricas recolectadas
- Resumen de carga
- Estimaciones en `.txt` y `.csv`
- GrÃ¡ficos generados en `images/`

Esta automatizaciÃ³n permite repetir el experimento de forma rÃ¡pida, consistente y sin errores manuales, incluso en diferentes entornos o microservicios.


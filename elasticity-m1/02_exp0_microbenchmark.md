# üß™ Microbenchmark de Elasticidad: NGINX

Este documento describe el experimento de microbenchmark realizado para estimar el consumo de CPU por solicitud y por usuario virtual (VU) en un microservicio NGINX. Este experimento constituye un **paso previo esencial** para estructurar correctamente los an√°lisis de elasticidad en escenarios m√°s complejos.

La estimaci√≥n de consumo de recursos bajo carga controlada permite convertir m√©tricas de carga abstractas (como el n√∫mero de usuarios virtuales o el n√∫mero de solicitudes) en una estimaci√≥n directa de **demanda de CPU en millicores**. Esta conversi√≥n es cr√≠tica para construir curvas de demanda esperada y, con base en ellas, poder comparar la oferta real del sistema durante los experimentos de elasticidad.

Se eligi√≥ NGINX como microservicio base por su **simplicidad, predictibilidad y baja latencia**, lo que permite centrarse en la din√°mica de consumo de recursos sin introducir variabilidad innecesaria.

El microbenchmark se ejecuta en el **mismo entorno de Kubernetes local** que se utiliza en los experimentos principales, incluyendo la presencia de un HPA (Horizontal Pod Autoscaler), lo que garantiza que los resultados obtenidos sean completamente consistentes con el resto del sistema.

---

## üéØ Objetivo del experimento

El prop√≥sito principal es estimar cu√°ntos **millicores de CPU** consume el sistema bajo condiciones de carga controlada. En particular, se busca obtener:

- üßÆ **Millicores por request**
- üë§ **Millicores por usuario virtual (VU)**

Estos valores servir√°n como entrada directa para calcular la demanda de recursos (*resource demand*) en los experimentos de elasticidad, y son clave para habilitar el c√°lculo de m√©tricas como:

- Precisi√≥n de escalado
- Tiempos acumulados en sub/sobreaprovisionamiento
- M√©tricas de elasticidad hacia arriba, hacia abajo y global

---

## üß© Descripci√≥n general del sistema

Este microbenchmark fue dise√±ado para ejecutarse en el mismo entorno de pruebas que los experimentos de elasticidad: un **cluster de Kubernetes local** con soporte para escalamiento autom√°tico mediante **Horizontal Pod Autoscaler (HPA)**. Aunque en esta prueba el escalamiento no se activa (por el bajo nivel de carga), el HPA permanece activo para mantener coherencia con el resto del sistema.

El flujo de software se basa en una arquitectura simple pero representativa:

| Componente         | Rol en el experimento                              |
|--------------------|----------------------------------------------------|
| **NGINX**          | Microservicio bajo prueba                          |
| **K6**             | Generador de carga controlada                      |
| **Kubernetes**     | Plataforma de orquestaci√≥n                         |
| **HPA**            | Escalador autom√°tico (activo pero sin efecto aqu√≠) |
| **Scripts Bash**   | Automatizaci√≥n de recolecci√≥n y an√°lisis           |
| **Python + Docker**| An√°lisis de resultados                             |

### üîπ Condiciones experimentales

- Carga constante: **10 usuarios virtuales (VUs)** durante **1 minuto**
- No se espera escalamiento: el uso de CPU no deber√≠a superar el umbral del HPA
- Observaci√≥n principal: consumo de CPU y n√∫mero de pods durante la prueba
- Recolecci√≥n peri√≥dica de m√©tricas: cada **10 segundos**

Esta configuraci√≥n permite capturar de manera precisa la relaci√≥n entre la carga generada y los recursos consumidos, bajo un entorno id√©ntico al que se utilizar√° en los an√°lisis de elasticidad posteriores.

---

## üîÑ Flujo del experimento

El microbenchmark sigue una secuencia clara y automatizada de pasos, cuyo objetivo es generar carga controlada, recolectar m√©tricas relevantes y procesarlas para obtener estimaciones clave.

| Paso | Descripci√≥n t√©cnica |
|------|---------------------|
| 1Ô∏è‚É£   | **Despliegue del microservicio**: se aplican los manifiestos de Kubernetes (`deployment.yaml` y `hpa.yaml`) para iniciar el pod de NGINX junto con su autoscaler. |
| 2Ô∏è‚É£   | **Inicio del recolector de m√©tricas**: un script en Bash ejecuta la captura peri√≥dica del uso de CPU por pod y el n√∫mero de r√©plicas activas, guardando esta informaci√≥n en CSV. |
| 3Ô∏è‚É£   | **Ejecuci√≥n de la carga**: se lanza el script `benchmark_test.js` con K6, simulando 10 usuarios virtuales generando solicitudes de forma constante durante 1 minuto. |
| 4Ô∏è‚É£   | **Finalizaci√≥n de la recolecci√≥n**: el script de captura se detiene despu√©s de la prueba, asegurando que se hayan registrado todas las m√©tricas necesarias. |
| 5Ô∏è‚É£   | **An√°lisis de resultados**: un contenedor Docker ejecuta el script `analyze_microbenchmark.py`, que combina las m√©tricas de uso de CPU con los resultados de K6 para calcular las estimaciones finales. |

Este flujo completo puede ejecutarse de forma autom√°tica mediante el script `exp0_microbenchmark.sh`, lo que asegura reproducibilidad y facilita futuras repeticiones del experimento.

---

## ‚öôÔ∏è Configuraciones utilizadas

A continuaci√≥n se detallan las configuraciones clave utilizadas durante el experimento. Todas ellas est√°n contenidas en la carpeta `files/microbenchmark/`.

---

### üì¶ Manifiestos de Kubernetes

Ubicaci√≥n: `files/microbenchmark/manifests/`

- **`deployment.yaml`**: Define el despliegue de un pod con NGINX. Establece los `requests` y `limits` de CPU requeridos por el HPA.
- **`hpa.yaml`**: Configura un Horizontal Pod Autoscaler con umbral de CPU objetivo del 25% y un rango de entre 1 y 10 r√©plicas. Aunque no se espera escalamiento en esta prueba, se mantiene activo por coherencia con el resto del sistema.

---

### üß™ Script de carga (K6)

Ubicaci√≥n: `files/microbenchmark/loadtest/benchmark_test.js`

Genera una carga constante de 10 usuarios virtuales durante 1 minuto. Este patr√≥n permite observar el comportamiento del consumo de recursos bajo condiciones estables, sin variaciones bruscas de tr√°fico.

---

### üßæ Recolecci√≥n de m√©tricas

Ubicaci√≥n: `files/microbenchmark/scripts/metric_collector_microbenchmark.sh`

Este script captura m√©tricas cada 10 segundos durante la prueba, incluyendo:

- Uso de CPU por pod (en millicores)
- Uso relativo de CPU (en porcentaje)
- N√∫mero de pods activos

Los datos recolectados se almacenan en un archivo CSV (`microbenchmark_metrics.csv`) para su posterior an√°lisis.

---

### üìä Scripts de an√°lisis y visualizaci√≥n (Python)

Ubicaci√≥n: `files/microbenchmark/analysis/`

Despu√©s de la ejecuci√≥n de la prueba, los datos recolectados son procesados autom√°ticamente por una serie de scripts en Python contenidos en esta carpeta. Su prop√≥sito es calcular m√©tricas clave y generar visualizaciones √∫tiles para evaluar el consumo de recursos.

- **`analyze_microbenchmark.py`**  
  Script principal de an√°lisis. Lee el resumen de carga (`k6_summary.json`) y las m√©tricas del sistema (`microbenchmark_metrics.csv`) para calcular:
  - CPU total utilizada (millicores)
  - CPU por request
  - CPU por VU  
  Los resultados se exportan como archivos de texto (`microbenchmark_summary.txt`) y CSV (`microbenchmark_summary.csv`).

- **`plot_cpu_usage.py`**  
  Genera una gr√°fica temporal del uso de CPU (en porcentaje) para los pods activos. Permite observar la estabilidad y carga efectiva del sistema.

- **`plot_pod_count.py`**  
  Produce una gr√°fica del n√∫mero de pods activos a lo largo del tiempo. Verifica que no haya habido escalamiento accidental durante la prueba.

- **`requirements.txt`**  
  Lista de dependencias necesarias para ejecutar los scripts (`pandas`, `matplotlib`, entre otras).

- **`Dockerfile`**  
  Define la imagen Docker utilizada para ejecutar todo el an√°lisis en un entorno limpio y reproducible. Esto elimina la necesidad de instalar dependencias localmente y asegura que los resultados sean consistentes.

---

Estas herramientas complementan el entorno de ejecuci√≥n, permitiendo transformar datos crudos en m√©tricas cuantificables y visualizaciones interpretables, todo de forma autom√°tica.

---

## üî¢ C√°lculo de estimaciones

El objetivo principal del microbenchmark es obtener estimaciones confiables sobre el consumo de CPU por unidad de carga. Estas estimaciones permiten transformar medidas abstractas como "cantidad de usuarios virtuales" o "n√∫mero de solicitudes" en una **demanda de recursos cuantificable en millicores**, lo que resulta indispensable para construir curvas de elasticidad y evaluar el desempe√±o del autoscaler.

El proceso de c√°lculo est√° totalmente automatizado mediante el script `analyze_microbenchmark.py`, que integra las m√©tricas recolectadas desde Kubernetes y los resultados de carga generados por K6.

---

### üìå Fuentes de datos utilizadas

- **`microbenchmark_metrics.csv`**  
  Contiene muestras del uso de CPU (en millicores) y el n√∫mero de pods activos recolectadas cada 10 segundos durante el experimento.

- **`k6_summary.json`**  
  Archivo JSON generado autom√°ticamente por K6, donde se registran:
  - El n√∫mero total de solicitudes procesadas (`requests`)
  - El n√∫mero m√°ximo de usuarios virtuales (`vus_max`)
  - La duraci√≥n total de la prueba

---

### üìê Estimaciones calculadas

A partir de estas dos fuentes, se derivan las siguientes m√©tricas clave:

#### 1. **CPU total utilizada**

Esta m√©trica representa la cantidad total de recursos de CPU consumidos (en millicores) durante todo el experimento. Se obtiene a partir del archivo `microbenchmark_metrics.csv`, donde se recolecta el uso de CPU por pod cada 10 segundos.

Para calcular esta m√©trica, se suman todos los valores registrados en la columna `cpu(millicores)`:

C√≥digo relevante del script `analyze_microbenchmark.py`:
```python
cpu_total_millicores = df["cpu(millicores)"].sum()
```

#### 2. **CPU por solicitud (millicores/request)**

Esta m√©trica estima cu√°ntos millicores de CPU se consumen, en promedio, por cada solicitud HTTP enviada al sistema. Es fundamental para generar curvas de demanda basadas en tr√°fico.

El valor del n√∫mero total de solicitudes (`total_requests`) se obtiene del archivo `k6_summary.json`, generado por la herramienta K6 al finalizar la prueba:
```python
total_requests = k6_data["metrics"]["http_reqs"]["count"]
```

C√°lculo:
```python
cpu_per_request = cpu_total_millicores / total_requests if total_requests else 0
```

Esta estimaci√≥n es √∫til para construir una curva de demanda basada en el volumen de tr√°fico (requests por segundo), independientemente del n√∫mero de usuarios virtuales.

#### 3. **CPU por usuario virtual (millicores/VU)**

Esta m√©trica estima el consumo promedio de CPU por cada usuario virtual activo durante la prueba. Es √∫til cuando se modela la carga en t√©rminos de usuarios concurrentes.

El n√∫mero m√°ximo de usuarios virtuales (`vus_max`) tambi√©n se extrae desde `k6_summary.json`:
```python
vus_max = int(k6_data["metrics"]["vus_max"]["value"])
```

C√°lculo:
```python
cpu_per_vu = cpu_total_millicores / vus_max if vus_max else 0
```

Resulta √∫til cuando la carga del sistema se modela en t√©rminos de usuarios concurrentes, como suele hacerse en herramientas de pruebas de carga o simulaciones de comportamiento de usuarios.

---

### üìù Exportaci√≥n de resultados

Una vez calculadas, las estimaciones son exportadas en los siguientes formatos:

- **`microbenchmark_summary.txt`**: resumen legible para revisi√≥n manual o presentaci√≥n
- **`microbenchmark_summary.csv`**: archivo estructurado en formato tabular para uso automatizado en experimentos posteriores

Estas salidas sirven como **insumo directo** para calcular la *demanda de recursos* en experimentos de elasticidad, permitiendo generar curvas te√≥ricas de demanda que luego se comparan con la oferta observada por el sistema en producci√≥n.

---

### üí° Relevancia del c√°lculo

Este proceso es fundamental porque permite estimar, de forma cuantificable, la demanda de CPU asociada a una carga determinada. Sin estas m√©tricas, no ser√≠a posible transformar el comportamiento observado del sistema (usuarios o requests) en una estimaci√≥n concreta del consumo de recursos. Estas estimaciones son el insumo base para evaluar si el sistema est√° correctamente aprovisionado en pruebas de elasticidad.


---

## üìä Resultados obtenidos

Tras ejecutar el microbenchmark sobre el microservicio NGINX, se obtuvieron los siguientes valores a partir del an√°lisis autom√°tico realizado por el script `analyze_microbenchmark.py`.

> ‚ö†Ô∏è **Importante:** Estos valores corresponden a una ejecuci√≥n espec√≠fica sobre un microservicio concreto (NGINX) bajo carga controlada. Los resultados pueden variar significativamente dependiendo del tipo de aplicaci√≥n, su comportamiento interno, la configuraci√≥n de recursos y el entorno de despliegue. Por lo tanto, se recomienda repetir este microbenchmark para cada microservicio que se desee evaluar.

---

### üìà Resumen num√©rico

| M√©trica                     | Valor                          | Interpretaci√≥n |
|----------------------------|--------------------------------|----------------|
| Total de requests          | 303 solicitudes                | Cantidad total de solicitudes procesadas durante el experimento. |
| M√°ximo de usuarios virtuales (VU) | 10                            | N√∫mero de VUs activos generando carga durante toda la prueba. |
| CPU total utilizada        | 15.00 millicores               | Suma del uso de CPU reportado por Kubernetes a lo largo de la prueba. |
| CPU por solicitud          | 0.05 millicores/request        | Promedio de millicores necesarios para atender una solicitud individual. |
| CPU por usuario virtual    | 1.50 millicores/VU             | Promedio de millicores consumidos por cada VU activo. |

Estas m√©tricas permiten cuantificar la *demanda de recursos* del microservicio y son fundamentales para modelar correctamente escenarios de elasticidad.

---

### üìä Gr√°ficas generadas

A continuaci√≥n se analizan las gr√°ficas generadas durante la ejecuci√≥n:

---

#### üîπ N√∫mero de pods activos

![Evoluci√≥n de pods](images/microbenchmark/pod_count_over_time.png)

Este gr√°fico muestra que durante toda la duraci√≥n del experimento el n√∫mero de pods activos fue constante e igual a **1**. Esto era esperable, ya que:

- El HPA estaba configurado pero no se alcanz√≥ el umbral de uso de CPU (25%) que desencadenar√≠a el escalado.
- La carga generada fue deliberadamente baja (10 VUs) para mantener el sistema estable.

**Interpretaci√≥n:**  
La ausencia de escalamiento confirma que el sistema oper√≥ en un rango de carga estable y bajo aprovisionamiento controlado, ideal para extraer mediciones precisas sin interferencias.

---

#### üîπ Uso relativo de CPU (%)

![Uso de CPU por pod](images/microbenchmark/cpu_usage_per_pod.png)

Este gr√°fico muestra el porcentaje de uso de CPU del pod de NGINX a lo largo del tiempo.

- Se observa un aumento progresivo en el consumo, estabiliz√°ndose alrededor del **3% de uso** en la segunda mitad de la prueba.
- La ca√≠da al final corresponde al t√©rmino de la carga generada.

**Interpretaci√≥n:**  
El consumo gradual de CPU refleja un comportamiento estable y predecible del microservicio. La baja utilizaci√≥n confirma que la carga generada no exigi√≥ el escalamiento, pero s√≠ permiti√≥ detectar consumo real, suficiente para derivar las m√©tricas de demanda.

---

Los resultados num√©ricos y visuales muestran que el sistema respondi√≥ de manera estable y controlada, lo que valida la calidad del experimento. Las m√©tricas obtenidas permitir√°n construir curvas de demanda realistas en futuras pruebas de elasticidad y calcular indicadores como la precisi√≥n del escalado y los tiempos de sobre/subaprovisionamiento.

---

## üìÅ Estructura del experimento

A continuaci√≥n se muestra la organizaci√≥n del experimento dentro del directorio `files/microbenchmark/`. Esta estructura agrupa todos los componentes necesarios para ejecutar, monitorear y analizar el microbenchmark de forma modular y ordenada.

```
files/microbenchmark/
‚îú‚îÄ‚îÄ manifests/                            # Archivos YAML para despliegue
‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml                   # Deployment del pod nginx con requests/limits
‚îÇ   ‚îî‚îÄ‚îÄ hpa.yaml                          # Autoscaler HPA con CPU objetivo = 25%
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ metric_collector_microbenchmark.sh # Recolector de CPU y n. de pods
‚îÇ   ‚îî‚îÄ‚îÄ benchmark_test.js                # Script k6 para generar carga controlada
‚îú‚îÄ‚îÄ files/        
‚îÇ   ‚îî‚îÄ‚îÄ                                     
‚îî‚îÄ‚îÄ analysis/
    ‚îú‚îÄ‚îÄ files/
    ‚îÇ   ‚îú‚îÄ‚îÄ microbenchmark_metrics.csv        
    ‚îÇ   ‚îú‚îÄ‚îÄ microbenchmark_summary.txt        
    ‚îú‚îÄ‚îÄ images/
    ‚îÇ   ‚îú‚îÄ‚îÄ cpu_pod/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pod1_cpu.png
    ‚îÇ   ‚îú‚îÄ‚îÄ cpu_usage_per_pod.png           # Grafica sobre el uso de CPU por pod 
    ‚îÇ   ‚îî‚îÄ‚îÄ pod_count_over_time.png        # Grafica de pods a traves del tiempo
    ‚îú‚îÄ‚îÄ plot_cpu_usage.py                 # Genera gr√°fico de uso de CPU por pod
    ‚îú‚îÄ‚îÄ plot_pod_count.py                 # Gr√°fico del n. de pods activos
    ‚îú‚îÄ‚îÄ analyze_microbenchmark.py         # Calcula CPU/request y CPU/VU
    ‚îú‚îÄ‚îÄ requirements.txt                  # Dependencias Python
    ‚îî‚îÄ‚îÄ Dockerfile                        # Contenedor para ejecutar an√°lisis
```

Esta organizaci√≥n permite:
- Acceder de forma ordenada a m√©tricas, resultados y visualizaciones
- Facilitar el mantenimiento y reutilizaci√≥n del experimento para otros microservicios

---

## ü§ñ Automatizaci√≥n del proceso

La ejecuci√≥n del microbenchmark puede realizarse de forma manual o completamente automatizada mediante un script Bash. A continuaci√≥n se presentan ambas alternativas.

---

### üß≠ Ejecuci√≥n manual paso a paso

Si se desea ejecutar el microbenchmark sin utilizar automatizaci√≥n, los siguientes comandos deben ejecutarse en orden para replicar el flujo completo:

1. **Aplicar manifiestos en Kubernetes:**
   ```bash
   kubectl apply -f microbenchmark/manifests/deployment.yaml
   kubectl apply -f microbenchmark/manifests/hpa.yaml
   ```
   Esto despliega el pod NGINX con configuraci√≥n de CPU y activa el HPA.

2. **Esperar a que el pod se inicialice correctamente:**
   ```bash
   sleep 20
   ```

3. **Obtener la IP del cl√∫ster y actualizar el script de carga:**
   ```bash
   kubectl get nodes -o wide
   # Editar manualmente 'benchmark_test.js' y reemplazar <IP_DEL_CLUSTER>
   ```

4. **Iniciar el recolector de m√©tricas en segundo plano:**
   ```bash
   bash microbenchmark/scripts/metric_collector_microbenchmark.sh &
   METRIC_PID=$!
   ```

5. **Ejecutar la prueba con K6:**
   ```bash
   k6 run --summary-export microbenchmark/output/k6_summary.json \
     microbenchmark/scripts/benchmark_test.js
   ```

6. **Esperar para capturar la estabilizaci√≥n post-carga:**
   ```bash
   sleep 30
   ```

7. **Finalizar el recolector de m√©tricas:**
   ```bash
   kill $METRIC_PID
   ```

8. **Ejecutar an√°lisis autom√°tico con Docker:**
   ```bash
   docker build -t microbenchmark-analysis microbenchmark/analysis
   docker run --rm \
     -v "$(pwd)/microbenchmark/output:/app/output" \
     -v "$(pwd)/microbenchmark/analysis/files:/app/files" \
     -v "$(pwd)/microbenchmark/analysis/images:/app/images" \
     microbenchmark-analysis
   ```

9. **Eliminar los recursos desplegados en Kubernetes:**
   ```bash
   kubectl delete -f microbenchmark/manifests/deployment.yaml
   kubectl delete -f microbenchmark/manifests/hpa.yaml
   ```

---

### üß™ Ejecuci√≥n autom√°tica con `exp0_microbenchmark.sh`

Todo el flujo descrito anteriormente ha sido encapsulado en el script `exp0_microbenchmark.sh`, ubicado en la ra√≠z del directorio `files/`.

Este script realiza las siguientes acciones:

1. Despliega NGINX y su HPA en Kubernetes.
2. Solicita al usuario que actualice la IP en el script de carga.
3. Inicia el recolector de m√©tricas.
4. Ejecuta la prueba con K6.
5. Detiene el recolector.
6. Ejecuta el an√°lisis completo en Docker.
7. Limpia los recursos del cl√∫ster.

#### üìç Ubicaci√≥n del script

```
files/exp0_microbenchmark.sh
```

#### ‚ñ∂Ô∏è Instrucciones de uso

> ‚ö†Ô∏è **Requisito importante:** el script debe ejecutarse desde la ra√≠z del proyecto (`elasticity-m1`) para que las rutas relativas funcionen correctamente.

```bash
cd elasticity-m1/files
bash exp0_microbenchmark.sh
```

Al finalizar, el script imprime la ubicaci√≥n de todos los archivos generados:

- CSV de m√©tricas recolectadas
- Resumen de carga
- Estimaciones en `.txt` y `.csv`
- Gr√°ficos generados en `images/`

Esta automatizaci√≥n permite repetir el experimento de forma r√°pida, consistente y sin errores manuales, incluso en diferentes entornos o microservicios.


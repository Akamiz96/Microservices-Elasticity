# ------------------------------------------------------------------------------
# ARCHIVO: calculate_elasticity_metrics.py
# DESCRIPCI√ìN: C√°lculo de m√©tricas de elasticidad a partir de la demanda y la
#              oferta de CPU en un experimento de escalamiento autom√°tico.
#
# AUTOR: Alejandro Castro Mart√≠nez
# FECHA DE MODIFICACI√ìN: 5 de abril de 2025
# CONTEXTO:
#   - El script estima elasticidades y m√©tricas relacionadas para dos tipos de demanda:
#       (1) en funci√≥n de los VUs (usuarios virtuales)
#       (2) en funci√≥n del n√∫mero de requests.
#   - Utiliza los resultados del microbenchmark para convertir demanda en CPU.
#   - Guarda los resultados en archivos de texto para cada tipo de demanda.
# ------------------------------------------------------------------------------

import pandas as pd
import json
from datetime import datetime, timedelta
import numpy as np

# ============================================================================
# CONFIGURACI√ìN DEL EXPERIMENTO Y PAR√ÅMETROS
# ============================================================================

cpu_per_vu = 1.50    # millicores por VU (estimado en microbenchmark)
cpu_per_req = 0.05   # millicores por request (estimado en microbenchmark)
requests_per_vu_per_second = 1  # tasa asumida de requests por VU por segundo
sampling_interval = 10          # intervalo de muestreo en segundos
reconfig_threshold = 30         # umbral para agrupar eventos de escalamiento

# ============================================================================
# FUNCI√ìN AUXILIAR: calcular tiempo de reconfiguraci√≥n (agrupado por tipo)
# ============================================================================

def calcular_tiempo_reconfiguracion(events_csv, threshold_seconds=30):
    """
    Agrupa eventos de escalamiento cercanos en el tiempo y calcula la duraci√≥n
    total de los bloques de reconfiguraci√≥n por tipo.

    Par√°metros:
    - events_csv (str): Ruta al CSV con eventos de escalamiento.
    - threshold_seconds (int): Umbral m√°ximo de separaci√≥n para agrupar eventos.

    Retorna:
    - Œ∏_up (float): Tiempo total en segundos en bloques de escalamiento hacia arriba.
    - Œ∏_down (float): Tiempo total en segundos en bloques de escalamiento hacia abajo.
    - df_bloques (pd.DataFrame): Informaci√≥n detallada de cada bloque agrupado.
    """
    df = pd.read_csv(events_csv, parse_dates=["timestamp"])
    df = df.sort_values("timestamp")
    resultados = []

    for tipo in ["scaleup", "scaledown"]:
        sub = df[df["scale_action"] == tipo].copy()
        sub["delta"] = sub["timestamp"].diff().dt.total_seconds().fillna(threshold_seconds + 1)
        grupo_id = 0
        grupo_ids = []

        for delta in sub["delta"]:
            if delta > threshold_seconds:
                grupo_id += 1
            grupo_ids.append(grupo_id)

        sub["grupo"] = grupo_ids
        for grupo, grupo_df in sub.groupby("grupo"):
            start = grupo_df["timestamp"].min()
            end = grupo_df["timestamp"].max()
            duracion = (end - start).total_seconds()
            resultados.append({
                "tipo": tipo,
                "grupo": grupo,
                "start": start,
                "end": end,
                "duracion": duracion,
                "eventos": len(grupo_df)
            })

    df_bloques = pd.DataFrame(resultados)
    Œ∏_up = df_bloques[df_bloques["tipo"] == "scaleup"]["duracion"].sum()
    Œ∏_down = df_bloques[df_bloques["tipo"] == "scaledown"]["duracion"].sum()
    return Œ∏_up, Œ∏_down, df_bloques

# ============================================================================
# FUNCI√ìN PRINCIPAL DE C√ÅLCULO DE M√âTRICAS Y GUARDADO DE RESULTADOS
# ============================================================================

def calcular_metricas(df_demand, label, output_path, Œ∏_up, Œ∏_down, df_bloques):
    """
    Calcula todas las m√©tricas de elasticidad a partir de la oferta y la demanda.

    Par√°metros:
    - df_demand (pd.DataFrame): Serie temporal con la demanda (millicores).
    - label (str): Etiqueta para identificar el tipo de demanda en los resultados.
    - output_path (str): Ruta al archivo de salida de texto.
    - Œ∏_up (float): Tiempo total en bloques de escalamiento hacia arriba.
    - Œ∏_down (float): Tiempo total en bloques de escalamiento hacia abajo.
    - df_bloques (pd.DataFrame): Detalles de bloques de reconfiguraci√≥n.

    Retorna:
    - None. Escribe resultados en el archivo especificado.
    """
    df_comb = pd.merge_asof(
        df_demand.sort_values("timestamp"),
        df_supply,
        on="timestamp",
        direction="nearest",
        tolerance=pd.Timedelta(seconds=sampling_interval)
    )
    df_comb.dropna(inplace=True)

    # Duraci√≥n total del experimento
    T = df_comb["timestamp"].max() - df_comb["timestamp"].min()
    T_seconds = T.total_seconds()
    T_minutes = T_seconds / 60

    # ------------------------
    # SUB/SOBREAPROVISIONAMIENTO
    # ------------------------

    # Subaprovisionamiento: cuando la demanda supera la oferta
    df_comb["under"] = df_comb["demand"] - df_comb["supply"]
    df_comb["under"] = df_comb["under"].apply(lambda x: x if x > 0 else 0)

    # Sobreaprovisionamiento: cuando la oferta supera la demanda
    df_comb["over"] = df_comb["supply"] - df_comb["demand"]
    df_comb["over"] = df_comb["over"].apply(lambda x: x if x > 0 else 0)

    # Œ£U y Œ£O son las √°reas acumuladas de sub/sobreaprovisionamiento (millicores * segundos)
    Œ£U = df_comb["under"].sum() * sampling_interval
    Œ£O = df_comb["over"].sum() * sampling_interval

    # ------------------------
    # TIEMPOS Y PROMEDIOS
    # ------------------------

    under_periods = df_comb[df_comb["under"] > 0]
    over_periods = df_comb[df_comb["over"] > 0]

    # Œ£A y Œ£B son los tiempos totales en los que hubo sub/sobreaprovisionamiento
    Œ£A = len(under_periods) * sampling_interval
    Œ£B = len(over_periods) * sampling_interval

    # AÃÖ y BÃÖ: proporci√≥n del tiempo total que se pas√≥ en cada estado
    A_bar = Œ£A / T_seconds if T_seconds > 0 else 0
    B_bar = Œ£B / T_seconds if T_seconds > 0 else 0

    # ≈™ y ≈å: intensidad media de sub/sobreaprovisionamiento durante sus per√≠odos
    U_bar = Œ£U / Œ£A if Œ£A > 0 else 0
    O_bar = Œ£O / Œ£B if Œ£B > 0 else 0

    # ------------------------
    # PRECISI√ìN
    # ------------------------

    # P·µ§ y Pùëë: precisi√≥n del sistema para evitar sub/sobreaprovisionamiento
    # Mientras menores sean, mejor se est√° ajustando la oferta a la demanda
    P_u = Œ£U / T_seconds if T_seconds > 0 else 0
    P_d = Œ£O / T_seconds if T_seconds > 0 else 0

    # ------------------------
    # ELASTICIDADES PARCIALES
    # ------------------------

    # E·µ§ = 1 / (AÃÖ √ó ≈™)
    E_u = 1 / (A_bar * U_bar) if A_bar * U_bar > 0 else 0

    # Eùëë = 1 / (BÃÖ √ó ≈å)
    E_d = 1 / (B_bar * O_bar) if B_bar * O_bar > 0 else 0

    # ------------------------
    # ELASTICIDAD TOTAL
    # ------------------------

    Œ∏ = Œ∏_up + Œ∏_down  # tiempo total de reconfiguraci√≥n

    # Œº: intensidad media de desajuste respecto al total de muestras
    Œº = (Œ£U + Œ£O) / (T_seconds * len(df_comb)) if len(df_comb) > 0 else 0

    # E‚Çó: mide la capacidad global del sistema para ajustarse de manera eficiente
    E_l = 1 / (Œ∏ * Œº) if Œ∏ * Œº > 0 else 0

    # -------------------------------------------------------------------------------
    # M√âTRICAS COMPLEMENTARIAS
    # -------------------------------------------------------------------------------

    # E_global: elasticidad complementaria basada en penalizaci√≥n simple
    # Mide qu√© tan bien se evita tanto sub como sobreaprovisionamiento.
    # Valor ideal cercano a 1. Disminuye si hay mucho desajuste.
    E_global = 1 - ((Œ£U + Œ£O) / T_seconds) if T_seconds > 0 else 0

    # R_U: tasa promedio de subaprovisionamiento por segundo
    # √ötil para comparar la intensidad del desajuste en relaci√≥n al tiempo.
    R_U = Œ£U / T_seconds if T_seconds > 0 else 0

    # R_O: tasa promedio de sobreaprovisionamiento por segundo
    R_O = Œ£O / T_seconds if T_seconds > 0 else 0

    # theta_pct: porcentaje del tiempo total dedicado a reconfiguraciones
    # Muestra cu√°nto del experimento se us√≥ activamente en escalar.
    theta_pct = (Œ∏ / T_seconds) * 100 if T_seconds > 0 else 0

    # theta_up_pct y theta_down_pct: porcentaje de tiempo en escalamiento hacia arriba y hacia abajo
    theta_up_pct = (Œ∏_up / T_seconds) * 100 if T_seconds > 0 else 0
    theta_down_pct = (Œ∏_down / T_seconds) * 100 if T_seconds > 0 else 0

    # tiempo_util: porcentaje de tiempo √∫til del sistema (sin reconfigurar)
    tiempo_util = 100 - theta_pct


    # ------------------------
    # SALIDA DE RESULTADOS
    # ------------------------

    with open(output_path, "w") as f:
        f.write(f"=== M√âTRICAS DE ELASTICIDAD ({label}) ===\n\n")
        f.write("1. Duraci√≥n del per√≠odo de evaluaci√≥n (T):\n")
        f.write(f"   - Desde: {df_comb['timestamp'].min()}\n")
        f.write(f"   - Hasta: {df_comb['timestamp'].max()}\n")
        f.write(f"   - Total: {T} ({T_seconds:.2f} s, {T_minutes:.2f} min)\n\n")

        f.write("2. Recursos acumulados:\n")
        f.write(f"   - Œ£U (subaprovisionamiento): {Œ£U:.2f} millicore-s\n")
        f.write(f"   - Œ£O (sobreaprovisionamiento): {Œ£O:.2f} millicore-s\n\n")

        f.write("3. Tiempo en reconfiguraci√≥n (Œ∏):\n")
        f.write(f"   - Total: {Œ∏:.2f} s ({Œ∏/60:.2f} min)\n")
        f.write(f"   -   ScaleUp: {Œ∏_up:.2f} s\n")
        f.write(f"   -   ScaleDown: {Œ∏_down:.2f} s\n\n")

        f.write("4. Tiempos en estados de (sub/sobre)aprovisionamiento:\n")
        f.write(f"   - Œ£A (sub): {Œ£A:.2f} s |  AÃÖ: {A_bar:.2f} | ≈™: {U_bar:.2f} millicores\n")
        f.write(f"   - Œ£B (sobre): {Œ£B:.2f} s |  BÃÖ: {B_bar:.2f} | ≈å: {O_bar:.2f} millicores\n\n")

        f.write("5. M√©tricas derivadas:\n")
        f.write(f"   - Precisi√≥n de escalado hacia arriba (P·µ§): {P_u:.4f} millicore/s\n")
        f.write(f"   - Precisi√≥n de escalado hacia abajo (Pùëë): {P_d:.4f} millicore/s\n")
        f.write(f"   - Elasticidad de escalado hacia arriba (E·µ§): {E_u:.4f}\n")
        f.write(f"   - Elasticidad de escalado hacia abajo (Eùëë): {E_d:.4f}\n")
        f.write(f"   - Elasticidad total (E‚Çó): {E_l:.4f}\n\n")

        f.write("6. Bloques de reconfiguraci√≥n detectados:\n")
        for _, row in df_bloques.iterrows():
            f.write(f"   - [{row['tipo']}] {row['start']} ‚Üí {row['end']} | {row['duracion']:.1f} s, {row['eventos']} eventos\n")
        
        f.write("7. M√©tricas complementarias:\n")
        f.write(f"   - Elasticidad total (E): {E_global:.4f}\n")
        f.write(f"   - Subaprovisionamiento relativo (R_U): {R_U:.4f} millicore/s\n")
        f.write(f"   - Sobreaprovisionamiento relativo (R_O): {R_O:.4f} millicore/s\n")
        f.write(f"   - Porcentaje de tiempo en reconfiguraci√≥n (Œ∏%%): {theta_pct:.2f} %\n")
        f.write(f"       - ScaleUp: {theta_up_pct:.2f} %\n")
        f.write(f"       - ScaleDown: {theta_down_pct:.2f} %\n")
        f.write(f"   - Porcentaje de tiempo √∫til: {tiempo_util:.2f} %\n\n")

# ============================================================================
# ETAPA 1: Cargar m√©tricas de Kubernetes (oferta de CPU)
# ============================================================================

df_metrics = pd.read_csv("output/basic_metrics.csv")
df_metrics["timestamp"] = pd.to_datetime(df_metrics["timestamp"])
df_metrics["cpu(millicores)"] = pd.to_numeric(df_metrics["cpu(millicores)"], errors="coerce")
df_supply = df_metrics.groupby("timestamp")["cpu(millicores)"].sum().reset_index()
df_supply.rename(columns={"cpu(millicores)": "supply"}, inplace=True)

# ============================================================================
# ETAPA 2: Leer tiempo de inicio de k6 y etapas definidas
# ============================================================================

with open("output/k6_start_time.txt") as f:
    k6_start_time = datetime.strptime(f.read().strip(), "%Y-%m-%d %H:%M:%S")

with open("output/stages.json") as f:
    stages = json.load(f)

# ============================================================================
# ETAPA 3: Generar serie de demanda en funci√≥n de VUs y Requests
# ============================================================================

def parse_duration(d): return int(d[:-1]) * (60 if d.endswith("m") else 1)

vus_series = []
current_time = k6_start_time
prev_target = 0

for stage in stages:
    duration = parse_duration(stage["duration"])
    target = stage["target"]
    for t in range(0, duration, sampling_interval):
        progress = t / duration
        vus = int(prev_target + (target - prev_target) * progress)
        vus_series.append({
            "timestamp": current_time + timedelta(seconds=t),
            "vus": vus,
            "reqs": vus * requests_per_vu_per_second * sampling_interval
        })
    prev_target = target
    current_time += timedelta(seconds=duration)

df_vus = pd.DataFrame(vus_series)

# ============================================================================
# ETAPA 4: Calcular bloques de reconfiguraci√≥n a partir de eventos
# ============================================================================

Œ∏_up, Œ∏_down, df_bloques = calcular_tiempo_reconfiguracion(
    "output/scaling_events_clean.csv",
    threshold_seconds=reconfig_threshold
)

# ============================================================================
# ETAPA 5: Calcular m√©tricas usando ambas formas de demanda
# ============================================================================

# Por VUs
df_vus_vu = df_vus.copy()
df_vus_vu["demand"] = df_vus_vu["vus"] * cpu_per_vu
calcular_metricas(df_vus_vu, "Basado en VUs", "output/elasticity_metrics_vus.txt", Œ∏_up, Œ∏_down, df_bloques)

# Por requests
df_vus_req = df_vus.copy()
df_vus_req["demand"] = df_vus_req["reqs"] * cpu_per_req
calcular_metricas(df_vus_req, "Basado en Requests", "output/elasticity_metrics_requests.txt", Œ∏_up, Œ∏_down, df_bloques)

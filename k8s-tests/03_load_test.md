## ğŸš€ Proceso de EjecuciÃ³n de la Prueba de Carga

Este documento describe de forma detallada el proceso utilizado para diseÃ±ar, ejecutar y monitorear un experimento de prueba de carga, cuyo objetivo es observar el comportamiento de escalabilidad horizontal en un despliegue de **NGINX** utilizando el escalador automÃ¡tico de pods (**HPA**) en **Kubernetes**.

---

## ğŸ¯ 1. Objetivo del Experimento

El propÃ³sito principal de este experimento es evaluar la capacidad de Kubernetes para escalar automÃ¡ticamente el nÃºmero de pods en funciÃ³n del uso de CPU, mediante la implementaciÃ³n de un **Horizontal Pod Autoscaler (HPA)**.

### Objetivos EspecÃ­ficos
- ğŸ“ˆ Generar una carga progresiva que aumente y disminuya con el tiempo.
- ğŸ“Š Observar cÃ³mo el nÃºmero de pods del despliegue se ajusta automÃ¡ticamente.
- ğŸ“¥ Recolectar mÃ©tricas relevantes de uso de recursos (CPU y memoria).
- ğŸ‘ Visualizar el comportamiento del sistema antes, durante y despuÃ©s de la carga.

---

## ğŸ§± 2. Estructura del Entorno

El experimento estÃ¡ completamente contenido en la carpeta `load_test/`, ubicada dentro de `files/`.

```
load_test/
â”œâ”€â”€ manifests/       # Archivos YAML del despliegue y HPA
â”œâ”€â”€ scripts/         # Scripts de carga y recolecciÃ³n de mÃ©tricas
â”œâ”€â”€ output/          # Archivo CSV con las mÃ©tricas
â””â”€â”€ analysis/        # Scripts de anÃ¡lisis y visualizaciÃ³n en Python
```

---

## âš™ï¸ 3. ConfiguraciÃ³n de Kubernetes

Los recursos se configuran mediante archivos YAML ubicados en `load_test/manifests/`.

### 3.1 ğŸ“„ Despliegue de NGINX (`nginx-deployment.yaml`)

Este archivo define los objetos necesarios para desplegar NGINX y exponerlo al exterior mediante un servicio:

#### Deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-test
  template:
    metadata:
      labels:
        app: nginx-test
    spec:
      containers:
      - name: nginx
        image: nginx:1.25.3
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "500m"
```

ğŸ“ **ExplicaciÃ³n de secciones clave:**
- `replicas`: comienza con 1 pod. Este valor serÃ¡ controlado luego por el HPA.
- `selector.matchLabels`: define cÃ³mo Kubernetes empareja los pods con el deployment.
- `template.metadata.labels`: asigna etiquetas necesarias para que el Service y el HPA encuentren los pods.
- `image`: define la versiÃ³n de la imagen de NGINX utilizada.
- `resources.requests`: reserva mÃ­nima de recursos para el contenedor. Es fundamental para que el HPA pueda calcular la utilizaciÃ³n.
- `resources.limits`: establece los lÃ­mites mÃ¡ximos que el contenedor puede consumir.

#### Service
```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: NodePort
  selector:
    app: nginx-test
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080
```

ğŸ“ **ExplicaciÃ³n de secciones clave:**
- `type: NodePort`: expone el servicio a travÃ©s de un puerto accesible desde fuera del clÃºster.
- `selector`: se conecta a los pods etiquetados como `app: nginx-test`.
- `port`: define el puerto del servicio dentro del clÃºster.
- `targetPort`: puerto donde el contenedor escucha (coincide con `containerPort`).
- `nodePort`: expone el servicio en el nodo fÃ­sico (en este caso, el puerto 30080).

ğŸ“Œ **Diagrama Resumen de Componentes**

```
[Usuarios] â‡¨ [NodePort Service:30080] â‡¨ [Pods NGINX (Deployment)] â‡¨ [Monitoreo HPA â‡¨ CPU]
```

### 3.2 ğŸ“„ Escalador AutomÃ¡tico de Pods (`hpa.yaml`)

Este archivo configura un `HorizontalPodAutoscaler` que controla dinÃ¡micamente la cantidad de rÃ©plicas del deployment `nginx-test` segÃºn el uso de CPU.

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-test
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 25
```

ğŸ“ **ExplicaciÃ³n de secciones clave:**
- `scaleTargetRef`: especifica el Deployment que serÃ¡ monitoreado y escalado por el HPA.
- `minReplicas` y `maxReplicas`: indican el rango de pods que puede existir en cualquier momento.
- `metrics.resource`: define que la mÃ©trica a monitorear es el uso de CPU.
- `averageUtilization`: umbral objetivo de utilizaciÃ³n promedio de CPU (en porcentaje). Si se supera este umbral, el HPA escala hacia arriba.

ğŸ“‹ **Resumen de ConfiguraciÃ³n del HPA**

| ParÃ¡metro          | Valor          | DescripciÃ³n                                           |
|--------------------|----------------|-------------------------------------------------------|
| `minReplicas`      | 1              | MÃ­nimo nÃºmero de pods                                 |
| `maxReplicas`      | 10             | MÃ¡ximo nÃºmero de pods                                 |
| `target.cpu.util`  | 25%            | Umbral de utilizaciÃ³n de CPU para escalar             |
| `scaleTargetRef`   | nginx-test     | Deployment que serÃ¡ escalado                          |

### 3.3 ğŸš€ Despliegue de los Manifiestos

```bash
kubectl apply -f load_test/manifests/nginx-deployment.yaml
kubectl apply -f load_test/manifests/hpa.yaml
```

ğŸ” **VerificaciÃ³n:**
```bash
kubectl get deployments
kubectl get services
kubectl get hpa
```

> âš ï¸ Este proceso tambiÃ©n estÃ¡ automatizado dentro del script `03_load_test.sh` (paso 1).



## ğŸ”¥ 4. GeneraciÃ³n de Carga (`test.js`)

El archivo `test.js`, ubicado en la carpeta `load_test/scripts/`, define el comportamiento de la prueba de carga que serÃ¡ aplicada al servicio NGINX desplegado. Utiliza la herramienta `k6`, un framework especializado en pruebas de rendimiento, para simular mÃºltiples usuarios realizando solicitudes HTTP de forma progresiva.

```javascript
export let options = {
  stages: [
    { duration: '1m', target: 50 },
    { duration: '2m', target: 150 },
    { duration: '3m', target: 300 },
    { duration: '2m', target: 50 },
    { duration: '2m', target: 0 },
  ],
  thresholds: {
    http_req_duration: ['p(95)<500'],
    http_req_failed: ['rate<0.01'],
  },
};

export default function () {
  http.get('http://<IP_DEL_CLUSTER>:30080');
  sleep(1);
}
```

ğŸ“Œ **Importante:** reemplazar `<IP_DEL_CLUSTER>` por la IP del nodo donde estÃ¡ expuesto el servicio. Puede obtenerse mediante:

```bash
kubectl get nodes -o wide
```

> ğŸ›  Este paso estÃ¡ automatizado parcialmente por el script `03_load_test.sh`, pero la ediciÃ³n manual de la IP puede seguir siendo necesaria.

---

## ğŸ“¡ 5. RecolecciÃ³n de MÃ©tricas (`metric_collector.sh`)

Este script recolecta mÃ©tricas de CPU, memoria y nÃºmero de pods en ejecuciÃ³n cada 10 segundos durante la prueba de carga.

### Actividades:
- Usa `kubectl top pod` para capturar el uso actual de recursos.
- Extrae `requests.cpu` desde la definiciÃ³n del pod.
- Calcula el % de uso de CPU:

```
%cpu = (uso_actual_millicores / requests_cpu_millicores) * 100
```

- Cuenta los pods activos:
```bash
kubectl get pods --no-headers | wc -l
```

Todos los datos son formateados como CSV y guardados en `load_test/output/metrics.csv`.

### ğŸ§ª Ejemplo de Salida

```csv
timestamp,pod,cpu(millicores),memory(bytes),%cpu,num_pods
2025-03-25 16:14:41,nginx-test-xxx,0,16Mi,0,1
...
```

ğŸ“Œ **AutomatizaciÃ³n:** el script se ejecuta automÃ¡ticamente desde `03_load_test.sh` en segundo plano (paso 3) y se detiene al finalizar la prueba.

#### âš ï¸ Si queda corriendo
```bash
ps aux | grep metric_collector.sh
kill <PID>
```

---

## ğŸ“ˆ 6. AnÃ¡lisis de Resultados (Python)

### 6.1 `plot_cpu_usage.py`
Este script genera visualizaciones del uso de CPU para cada pod a lo largo del tiempo:
- GrÃ¡fico combinado de todos los pods.
- GrÃ¡ficos individuales por pod (`images/cpu_pod/`).

```python
file_path = "output/metrics.csv"
df = pd.read_csv(file_path)
df["timestamp"] = pd.to_datetime(df["timestamp"])
...
```

### 6.2 `plot_pod_count.py`
Genera un grÃ¡fico de la evoluciÃ³n del nÃºmero de pods:
```python
plt.plot(df["timestamp"], df["num_pods"])
plt.savefig("images/pod_count_over_time.png")
```

ğŸ“Œ Asegura tener instaladas las dependencias (`pandas`, `matplotlib`) y configurar rutas si cambias el CSV de lugar.

---

## ğŸ³ 7. AutomatizaciÃ³n con Docker

Para garantizar reproducibilidad del entorno de anÃ¡lisis:

```dockerfile
FROM python:3.10-slim
WORKDIR /app
COPY plot_*.py requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
RUN mkdir -p images output
CMD ["sh", "-c", "python plot_cpu_usage.py && python plot_pod_count.py"]
```

ğŸ“¦ El contenedor se lanza automÃ¡ticamente desde `03_load_test.sh` (paso 6) y monta volÃºmenes necesarios para el anÃ¡lisis.

---

## ğŸ§¹ 8. EliminaciÃ³n de Recursos del ClÃºster

```bash
kubectl delete -f load_test/manifests/nginx-deployment.yaml
kubectl delete -f load_test/manifests/hpa.yaml
```

> Esta operaciÃ³n estÃ¡ automatizada en el paso 7 de `03_load_test.sh`.

---

âœ… **Con este flujo completo, es posible simular carga, recolectar datos, analizarlos y automatizar el ciclo de prueba para validar la escalabilidad horizontal de un servicio en Kubernetes.**

